{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "922b6c28",
   "metadata": {},
   "source": [
    "# LangChain MongoDB Integration - Memory and Semantic Caching for RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc29d11",
   "metadata": {},
   "source": [
    "This notebook is a companion to the [Memory and Semantic Caching](https://www.mongodb.com/docs/atlas/atlas-vector-search/ai-integrations/langchain/memory-semantic-cache/) tutorial. Refer to the page for set-up instructions and detailed explanations.\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/mongodb/docs-notebooks/blob/main/ai-integrations/langchain-memory-and-semantic-caching.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a289ba35",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install --quiet --upgrade langchain langchain-community langchain-core langchain-mongodb langchain-openai pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c672ba1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"<api-key>\"\n",
    "ATLAS_CONNECTION_STRING = \"<connection-string>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8384c99d",
   "metadata": {},
   "source": [
    "## Configure the Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90ce770",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mongodb import MongoDBAtlasVectorSearch\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Use text-embedding-ada-002 since that's what was used to create embeddings in the movies dataset\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "# Create the vector store\n",
    "vector_store = MongoDBAtlasVectorSearch.from_connection_string(\n",
    "   connection_string = ATLAS_CONNECTION_STRING,\n",
    "   embedding = embedding_model,\n",
    "   namespace = \"sample_mflix.embedded_movies\",\n",
    "   text_key = \"plot\",\n",
    "   embedding_key = \"plot_embedding\",\n",
    "   relevance_score_fn = \"dotProduct\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf1bff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Use LangChain helper method to create the vector search index\n",
    "vector_store.create_vector_search_index(\n",
    "   dimensions = 1536 # The dimensions of the vector embeddings to be indexed\n",
    ")\n",
    "\n",
    "# Wait for the index to build (this can take around a minute)\n",
    "time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3b6654",
   "metadata": {},
   "source": [
    "## Implement RAG with Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55583167",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Define the model to use for chat completion\n",
    "model = ChatOpenAI(model = \"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3b0361",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mongodb.chat_message_histories import MongoDBChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "         \n",
    "# Define a function that gets the chat message history \n",
    "def get_session_history(session_id: str) -> MongoDBChatMessageHistory:\n",
    "    return MongoDBChatMessageHistory(\n",
    "        connection_string=ATLAS_CONNECTION_STRING,\n",
    "        session_id=session_id,\n",
    "        database_name=\"sample_mflix\",\n",
    "        collection_name=\"embedded_movies\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dfa896",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Create a prompt to generate standalone questions from follow-up questions\n",
    "standalone_system_prompt = \"\"\"\n",
    "  Given a chat history and a follow-up question, rephrase the follow-up question to be a standalone question.\n",
    "  Do NOT answer the question, just reformulate it if needed, otherwise return it as is.\n",
    "  Only return the final standalone question.\n",
    "\"\"\"\n",
    "\n",
    "standalone_question_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", standalone_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "# Parse output as a string\n",
    "parse_output = StrOutputParser()\n",
    "\n",
    "question_chain = standalone_question_prompt | model | parse_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ad7c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Create a retriever\n",
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "\n",
    "# Create a retriever chain that processes the question with history and retrieves documents\n",
    "retriever_chain = RunnablePassthrough.assign(\n",
    "    context=question_chain | retriever | (lambda docs: \"\\n\\n\".join([d.page_content for d in docs]))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15d460d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt template that includes the retrieved context and chat history\n",
    "rag_system_prompt = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", rag_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4401715b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the RAG chain\n",
    "rag_chain = (\n",
    "    retriever_chain\n",
    "    | rag_prompt\n",
    "    | model\n",
    "    | parse_output\n",
    ")\n",
    "\n",
    "# Wrap the chain with message history\n",
    "rag_with_memory = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"question\",\n",
    "    history_messages_key=\"history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2093d8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First question\n",
    "response1 = rag_with_memory.invoke(\n",
    "    {\"question\": \"What are some good science fiction movies?\"},\n",
    "    {\"configurable\": {\"session_id\": \"user123\"}}\n",
    ")\n",
    "print(response1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14513bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Follow-up question that references the previous question\n",
    "response2 = rag_with_memory.invoke(\n",
    "    {\"question\": \"Which one has the best special effects?\"},\n",
    "    {\"configurable\": {\"session_id\": \"user123\"}}\n",
    ")\n",
    "print(response2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b2c3c5",
   "metadata": {},
   "source": [
    "## Add Semantic Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594315fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mongodb.cache import MongoDBAtlasSemanticCache\n",
    "from langchain_core.globals import set_llm_cache\n",
    "\n",
    "# Configure the semantic cache\n",
    "set_llm_cache(MongoDBAtlasSemanticCache(\n",
    "    connection_string = ATLAS_CONNECTION_STRING,\n",
    "    database_name = \"sample_mflix\",\n",
    "    collection_name = \"semantic_cache\",\n",
    "    embedding = embedding_model,\n",
    "    index_name = \"vector_index\",\n",
    "    similarity_threshold = 0.85  # Adjust based on your requirements\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8063217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First query (not cached)\n",
    "start_time = time.time()\n",
    "result1 = rag_with_memory.invoke({\"question\": \"What are some movies about time travel?\"})\n",
    "end_time = time.time()\n",
    "print(f\"First query time: {end_time - start_time:.2f} seconds\")\n",
    "print(result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4b0318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Semantically similar query (cached)\n",
    "start_time = time.time()\n",
    "result2 = rag_with_memory.invoke({\"question\": \"Can you recommend films that involve time travel?\"})\n",
    "end_time = time.time()\n",
    "print(f\"Second query time: {end_time - start_time:.2f} seconds\")\n",
    "print(result2)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
