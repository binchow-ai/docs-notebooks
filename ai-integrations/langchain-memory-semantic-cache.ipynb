{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "922b6c28",
   "metadata": {},
   "source": [
    "# LangChain MongoDB Integration - Memory and Semantic Caching for RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc29d11",
   "metadata": {},
   "source": [
    "This notebook is a companion to the [Memory and Semantic Caching](https://www.mongodb.com/docs/atlas/atlas-vector-search/ai-integrations/langchain/memory-semantic-cache/) tutorial. Refer to the page for set-up instructions and detailed explanations.\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/mongodb/docs-notebooks/blob/main/ai-integrations/langchain-memory-and-semantic-caching.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a289ba35",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install --quiet --upgrade langchain langchain-community langchain-core langchain-mongodb langchain-voyageai langchain-openai pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c672ba1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"<openai-key>\"\n",
    "os.environ[\"VOYAGE_API_KEY\"] = \"<voyage-key>\"\n",
    "MONGODB_URI = \"<connection-string>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8384c99d",
   "metadata": {},
   "source": [
    "## Configure the Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90ce770",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mongodb import MongoDBAtlasVectorSearch\n",
    "from langchain_voyageai import VoyageAIEmbeddings\n",
    "\n",
    "# Use the voyage-3 embedding model\n",
    "embedding_model = VoyageAIEmbeddings(model=\"voyage-3\")\n",
    "\n",
    "# Create the vector store\n",
    "vector_store = MongoDBAtlasVectorSearch.from_connection_string(\n",
    "   connection_string = MONGODB_URI,\n",
    "   embedding = embedding_model,\n",
    "   namespace = \"langchain_db.rag_with_memory\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb2f164",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load the PDF\n",
    "loader = PyPDFLoader(\"https://investors.mongodb.com/node/13176/pdf\")\n",
    "data = loader.load()\n",
    "\n",
    "# Split PDF into documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=20)\n",
    "docs = text_splitter.split_documents(data)\n",
    "\n",
    "# Add data to the vector store\n",
    "vector_store.add_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf1bff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use helper method to create the vector search index\n",
    "vector_store.create_vector_search_index(\n",
    "   dimensions = 1024,       # The dimensions of the vector embeddings to be indexed\n",
    "   wait_until_complete = 60 # Number of seconds to wait for the index to build (can take around a minute)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3b6654",
   "metadata": {},
   "source": [
    "## Implement RAG with Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55583167",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Define the model to use for chat completion\n",
    "llm = ChatOpenAI(model = \"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3b0361",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mongodb.chat_message_histories import MongoDBChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "         \n",
    "# Define a function that gets the chat message history \n",
    "def get_session_history(session_id: str) -> MongoDBChatMessageHistory:\n",
    "    return MongoDBChatMessageHistory(\n",
    "        connection_string=MONGODB_URI,\n",
    "        session_id=session_id,\n",
    "        database_name=\"langchain_db\",\n",
    "        collection_name=\"rag_with_memory\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dfa896",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Create a prompt to generate standalone questions from follow-up questions\n",
    "standalone_system_prompt = \"\"\"\n",
    "  Given a chat history and a follow-up question, rephrase the follow-up question to be a standalone question.\n",
    "  Do NOT answer the question, just reformulate it if needed, otherwise return it as is.\n",
    "  Only return the final standalone question.\n",
    "\"\"\"\n",
    "\n",
    "standalone_question_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", standalone_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "# Parse output as a string\n",
    "parse_output = StrOutputParser()\n",
    "\n",
    "question_chain = standalone_question_prompt | llm | parse_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ad7c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Create a retriever\n",
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={ \"k\": 5 })\n",
    "\n",
    "# Create a retriever chain that processes the question with history and retrieves documents\n",
    "retriever_chain = RunnablePassthrough.assign(context=question_chain | retriever | (lambda docs: \"\\n\\n\".join([d.page_content for d in docs])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15d460d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt template that includes the retrieved context and chat history\n",
    "rag_system_prompt = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", rag_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4401715b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the RAG chain\n",
    "rag_chain = (\n",
    "    retriever_chain\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | parse_output\n",
    ")\n",
    "\n",
    "# Wrap the chain with message history\n",
    "rag_with_memory = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"question\",\n",
    "    history_messages_key=\"history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2093d8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First question\n",
    "response_1 = rag_with_memory.invoke(\n",
    "    {\"question\": \"What was MongoDB's latest acquisition?\"},\n",
    "    {\"configurable\": {\"session_id\": \"user_1\"}}\n",
    ")\n",
    "print(response_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14513bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Follow-up question that references the previous question\n",
    "response_2 = rag_with_memory.invoke(\n",
    "    {\"question\": \"Why did they do it?\"},\n",
    "    {\"configurable\": {\"session_id\": \"user_1\"}}\n",
    ")\n",
    "print(response_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b2c3c5",
   "metadata": {},
   "source": [
    "## Add Semantic Caching\n",
    "\n",
    "The semantic cache caches only the input to the LLM. When using it in retrieval chains, \n",
    "note that documents retrieved can change between runs, resulting in cache misses for \n",
    "semantically similar queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594315fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mongodb.cache import MongoDBAtlasSemanticCache\n",
    "from langchain_core.globals import set_llm_cache\n",
    "\n",
    "# Configure the semantic cache\n",
    "set_llm_cache(MongoDBAtlasSemanticCache(\n",
    "    connection_string = MONGODB_URI,\n",
    "    database_name = \"langchain_db\",\n",
    "    collection_name = \"semantic_cache\",\n",
    "    embedding = embedding_model,\n",
    "    index_name = \"vector_index\",\n",
    "    similarity_threshold = 0.5  # Adjust based on your requirements\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8063217",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# First query (not cached)\n",
    "rag_with_memory.invoke(\n",
    "  {\"question\": \"What was MongoDB's latest acquisition?\"},\n",
    "  {\"configurable\": {\"session_id\": \"user_2\"}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4b0318",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Second query (cached)\n",
    "rag_with_memory.invoke(\n",
    "  {\"question\": \"What company did MongoDB acquire recently?\"},\n",
    "  {\"configurable\": {\"session_id\": \"user_2\"}}\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
